#TODO: Uses some of this https://zeigren.com/posts/monitoring_prometheus_loki/
# my global config
global:
  scrape_interval:     5s # By default, scrape targets every 15 seconds.
  # evaluation_interval: 15s # By default, scrape targets every 15 seconds.
  # evaluation_interval: 5s # By default, scrape targets every 15 seconds.
  # scrape_timeout:     1m
  # scrape_timeout is set to the global default (10s).

  # Attach these labels to any time series or alerts when communicating with
  # external systems (federation, remote storage, Alertmanager).
  external_labels:
      monitor: 'codelab-monitor'

# Load and evaluate rules in this file every 'evaluation_interval' seconds.
rule_files:
  - 'alert.rules'

# alert
alerting:
  alertmanagers:
  - scheme: http
    static_configs:
    - targets:
      - "alertmanager:9093"

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
# scrape_configs:
#   - job_name: 'unifipoller'
#     scrape_interval: 30s
#     static_configs:
#       - targets:
#         - https://unifi.controller:8443
#         - https://another.controller:8443
#     metrics_path: /scrape
#     relabel_configs:
#      - source_labels: [__address__]
#        target_label: __param_target
#      - source_labels: [__param_target]
#        target_label: instance
#      - target_label: __address__
#        replacement: localhost:9130
# *********************************************************
# Replace localhost with the IP of your unpoller host, and replace unifi.controller and another.controller with the IPs of your controllers.
# *********************************************************
scrape_configs:
  - job_name: 'unifipoller'
    scrape_interval: 30s
    static_configs:
      - targets:
        - https://{{ usg_ip }}:8443
    metrics_path: /scrape
    relabel_configs:
    - source_labels: [__address__]
      target_label: __param_target
    - source_labels: [__param_target]
      target_label: instance
    - target_label: __address__
      replacement: {{ prometheus_server_ip }}:9130
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.


  - job_name: 'prometheus'

    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s

    static_configs:
        - targets: ['{{ prometheus_server_ip }}:9090']

  - job_name: 'node-exporter'
    metrics_path: /metrics

    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s

    static_configs:
        - targets: ['{{ prometheus_server_ip }}:9100']

  # - job_name: 'loki'
  #   metrics_path: /metrics

  #   # Override the global default and scrape targets from this job every 5 seconds.
  #   scrape_interval: 5s

  #   static_configs:
  #       - targets: ['{{ prometheus_server_ip }}:3100']

  # - job_name: 'promtail'
  #   metrics_path: /metrics

  #   # Override the global default and scrape targets from this job every 5 seconds.
  #   scrape_interval: 5s

  #   static_configs:
  #       - targets: ['{{ prometheus_server_ip }}:9080']

  - job_name: 'docker'
    metrics_path: /metrics

    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s

    static_configs:
        - targets: ['{{ prometheus_server_ip }}:9323']

  - job_name: 'cadvisor'
    metrics_path: /metrics

    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s

    static_configs:
        - targets: ['{{ prometheus_server_ip }}:8081']

  - job_name: 'snmp-exporter'
    static_configs:
    - targets: ['{{ nfs_ip }}']
    metrics_path: /snmp
    params:
      module: [synology]
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - source_labels: [__param_target]
        regex: (.*)
        replacement: ${1}:9116
        target_label: __address__

#   - job_name: nas-node-exporter
#     static_configs:
#     - targets: ['192.168.1.111:9100']

  # - job_name: traefik # Network metrics for internet facing services
  #   # https://doc.traefik.io/traefik/observability/metrics/overview/
  #   static_configs:
  #     - targets:
  #         - traefik:8080
  #       labels:
  #         host: {{ prometheus_server_ip }}
  #   metric_relabel_configs:
  #     # Only keep data that is used in graphs to reduce size
  #     - source_labels: [__name__]
  #       regex: traefik_service_requests_total|traefik_service_request_duration_seconds_sum|traefik_service_request_duration_seconds_count
  #       action: keep

  - job_name: 'netdata'

    metrics_path: /api/v1/allmetrics
    params:
      format: [ prometheus ]

    static_configs:
      - targets: ['{{ prometheus_server_ip }}:19999']

  - job_name: 'blackbox'
    metrics_path: /probe
    params:
      module: [http_2xx]  # Look for a HTTP 200 response.
    static_configs:
      - targets:
        - blackbox:9115    # Target to probe with http.

    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox:9115  # The blackbox exporter's real hostname:port.

  # NOTE: https://prometheus.io/docs/guides/multi-target-exporter/
  # To fix this, we will use relabeling. Relabeling is useful here because behind the scenes many things in Prometheus are configured with internal labels. The details are complicated and out of scope for this guide. Hence we will limit ourselves to the necessary. But if you want to know more check out this talk. For now it suffices if you understand this:

  # All labels starting with __ are dropped after the scrape. Most internal labels start with __.
  # You can set internal labels that are called __param_<name>. Those set URL parameter with the key <name> for the scrape request.
  # There is an internal label __address__ which is set by the targets under static_configs and whose value is the hostname for the scrape request. By default it is later used to set the value for the label instance, which is attached to each metric and tells you were the metrics came from.

  - job_name: 'ping'
    metrics_path: /probe
    scrape_interval: 5s
    params:
      module: [http_2xx]  # Look for a HTTP 200 response.
    file_sd_configs:
      - files:
        - pinghosts.yaml
    relabel_configs:
      - source_labels: [__address__]
        regex: '(.*);(.*);(.*);(.*)'  #first is the url, thus unique for instance
        target_label: instance
        replacement: $1
      - source_labels: [__address__]
        regex: '(.*);(.*);(.*);(.*)'  #second is humanname to use in charts
        target_label: humanname
        replacement: $2
      - source_labels: [__address__]
        regex: '(.*);(.*);(.*);(.*)'  #third state whether this is testing external or internal network
        target_label: routing
        replacement: $3
      - source_labels: [__address__]
        regex: '(.*);(.*);(.*);(.*)'  #fourth is which switch/router the device is connected to.
        target_label: switching
        replacement: $4
      - source_labels: [instance]
        target_label: __param_target
      - target_label: __address__
        # replacement: 192.168.1.102:9115  # The blackbox exporter's real hostname:port.
        replacement: {{ prometheus_server_ip }}:9115  # The blackbox exporter's real hostname:port.

  - job_name: 'loki'
    dns_sd_configs:
      - names:
          - loki-read
          - loki-write
        type: A
        port: 3100

  - job_name: 'promtail'
    dns_sd_configs:
      - names:
          - promtail
        type: A
        port: 9080

  - job_name: minio
    metrics_path: /minio/v2/metrics/cluster
    scheme: http
    static_configs:
    - targets: [minio:9000]
