auth_enabled: false

http_prefix:

server:
  # Limit on the number of concurrent streams for gRPC calls (0 = unlimited)
  # CLI flag: -server.grpc-max-concurrent-streams
  grpc_server_max_concurrent_streams: 1000
  http_listen_address: 0.0.0.0
  grpc_listen_address: 0.0.0.0
  http_listen_port: 3100
  grpc_listen_port: 9095
  # SOURCE: https://medium.com/lonto-digital-services-integrator/grafana-loki-configuration-nuances-2e9b94da4ac1
  grpc_server_max_recv_msg_size: 104857600  # 100 Mb
  grpc_server_max_send_msg_size: 104857600  # 100 Mb
  http_server_write_timeout: 310s
  http_server_read_timeout: 310s
  log_level: info

# SOURCE: https://community.grafana.com/t/loki-error-on-port-9095-error-contacting-scheduler/67263/2
common:
  #instance_interface_names:
  #- eth0
  #temp disable to see if it fixes the read servers
  #ring:
  #  instance_interface_names:
  #    - eth0
  #  kvstore:
  #    store: memberlist
  storage:
    s3:
      endpoint: minio:9000
      insecure: true
      bucketnames: loki-data
      access_key_id: loki
      secret_access_key: supersecret
      s3forcepathstyle: true
  compactor_address: http://loki-write:3100

memberlist:
  join_members: ["loki-read", "loki-write"]
  # join_members: ["loki-write"]
  dead_node_reclaim_time: 30s
  gossip_to_dead_nodes_time: 15s
  left_ingesters_timeout: 30s
  bind_addr: ['0.0.0.0']
  bind_port: 7946
  gossip_interval: 2s

distributor:
  ring:
    kvstore:
      store: memberlist
  rate_store:
    # The max number of concurrent requests to make to ingester stream apis
    # CLI flag: -distributor.rate-store.max-request-parallelism
    max_request_parallelism: 200

    # The interval on which distributors will update current stream rates from
    # ingesters
    # CLI flag: -distributor.rate-store.stream-rate-update-interval
    stream_rate_update_interval: 1s

    # Timeout for communication between distributors and any given ingester when
    # updating rates
    # CLI flag: -distributor.rate-store.ingester-request-timeout
    ingester_request_timeout: 500ms

ruler:
  ring:
    kvstore:
      store: memberlist
# ruler:
#   enable_api: true
#   wal:
#     dir: /tmp/ruler-wal
#   storage:
#     type: local
#     local:
#       directory: /loki/rules
#   rule_path: /tmp/prom-rules
#   remote_write:
#     enabled: true
#     clients:
#       local:
#           url: http://prometheus:9090/api/v1/write
#           queue_config:
#             # send immediately as soon as a sample is generated
#             capacity: 1
#             batch_send_deadline: 0s

schema_config:
  configs:
  - from: 2018-04-15
    store: boltdb-shipper
    object_store: s3
    schema: v11
    index:
      prefix: index_
      period: 24h

storage_config:
  index_queries_cache_config:
    embedded_cache:
      enabled: true
  boltdb_shipper:
    shared_store: s3
    active_index_directory: /tmp/index
    cache_location: /tmp/boltdb-cache
    resync_interval: 5s


limits_config:
  # Maximum number of concurrent tail requests.
  # CLI flag: -querier.max-concurrent-tail-requests
  max_concurrent_tail_requests: 20
  # Whether the ingestion rate limit should be applied individually to each
  # distributor instance (local), or evenly shared across the cluster (global).
  # The ingestion rate strategy cannot be overridden on a per-tenant basis.
  # - local: enforces the limit on a per distributor basis. The actual effective
  # rate limit will be N times higher, where N is the number of distributor
  # replicas.
  # - global: enforces the limit globally, configuring a per-distributor local
  # rate limiter as 'ingestion_rate / N', where N is the number of distributor
  # replicas (it's automatically adjusted if the number of replicas change). The
  # global strategy requires the distributors to form their own ring, which is
  # used to keep track of the current number of healthy distributor replicas.
  # CLI flag: -distributor.ingestion-rate-limit-strategy
  ingestion_rate_strategy: global
  query_timeout: 300s
  max_query_parallelism: 24
  split_queries_by_interval: 48h
  # increment_duplicate_timestamp: true
  max_cache_freshness_per_query: '10m'
  # Enforce every sample has a metric name.
  enforce_metric_name: false
  reject_old_samples: true
  # reject_old_samples_max_age: 30m
  reject_old_samples_max_age: 168h
  # ingestion_rate_mb: 10
  # ingestion_burst_size_mb: 20
  # Per-user ingestion rate limit in sample size per second. Units in MB.
  # CLI flag: -distributor.ingestion-rate-limit-mb
  ingestion_rate_mb: 20
  # Per-user allowed ingestion burst size (in sample size). Units in MB. The burst
  # size refers to the per-distributor local rate limiter even in the case of the
  # 'global' strategy, and should be set at least to the maximum logs size
  # expected in a single push request.
  # CLI flag: -distributor.ingestion-burst-size-mb
  ingestion_burst_size_mb: 30
  # parallelize queries in 15min intervals

  # Maximum byte rate per second per stream, also expressible in human readable
  # forms (1MB, 256KB, etc).
  # CLI flag: -ingester.per-stream-rate-limit
  per_stream_rate_limit: "3MB"
  # Maximum burst bytes per stream, also expressible in human readable forms (1MB,
  # 256KB, etc). This is how far above the rate limit a stream can 'burst' before
  # the stream is limited.
  # CLI flag: -ingester.per-stream-rate-limit-burst
  per_stream_rate_limit_burst: "10MB"

  # SOURCE: https://git.corp.adobe.com/experience-platform/skyline-ops/blob/868e3cf28a80468cabc970f8bd7ece759656baaf/configsets/loki-logging/loki_common.yaml.mustache
  # max_entries_limit_per_query: 100000
  # ingestion_rate_mb: 50
  # ingestion_burst_size_mb: 50
  # per_stream_rate_limit: 20M
  # per_stream_rate_limit_burst: 50M
  # Limit how far back in time series data and metadata can be queried, up until
  # lookback duration ago. This limit is enforced in the query frontend, the
  # querier and the ruler. If the requested time range is outside the allowed
  # range, the request will not fail, but will be modified to only query data
  # within the allowed time range. The default value of 0 does not set a limit.
  # CLI flag: -querier.max-query-lookback
  max_query_lookback: 336h


chunk_store_config:
  chunk_cache_config:
    embedded_cache:
      enabled: true

  max_look_back_period: 336h
  #embedded_cache:
  #  enabled: true
  #  max_size_mb: 500

query_range:
  results_cache:
    cache:
      embedded_cache:
        enabled: true
  # make queries more cache-able by aligning them with their step intervals
  # Mutate incoming queries to align their start and end with their step.
  # CLI flag: -querier.align-querier-with-step
  align_queries_with_step: true
  # Maximum number of retries for a single request; beyond this, the downstream
  # error is returned.
  # CLI flag: -querier.max-retries-per-request
  max_retries: 5
  # Perform query parallelisations based on storage sharding configuration and
  # query ASTs. This feature is supported only by the chunks storage engine.
  # CLI flag: -querier.parallelise-shardable-queries
  parallelise_shardable_queries: true
  # Cache query results.
  # CLI flag: -querier.cache-results
  cache_results: true
  # SOURCE: https://git.corp.adobe.com/experience-platform/skyline-ops/blob/868e3cf28a80468cabc970f8bd7ece759656baaf/configsets/loki-logging/loki_common.yaml.mustache
  # parallelize queries in 15min intervals
  # split_queries_by_interval: 15m
  # cache_results: true

  # # SOURCE: https://git.corp.adobe.com/experience-platform/skyline-ops/blob/# 868e3cf28a80468cabc970f8bd7ece759656baaf/configsets/loki-logging/loki_common.yaml.mustache
  # results_cache:
  #   cache:
  #     # We're going to use the in-process "FIFO" cache
  #     enable_fifocache: true
  #     fifocache:
  #       size: 1024
  #       validity: 24h

frontend:
  log_queries_longer_than: 5s
  compress_responses: true
  max_outstanding_per_tenant: 2048

  # Max body size for downstream prometheus.
  # CLI flag: -frontend.max-body-size
  max_body_size: 104857600 # 100mb
  # True to enable query statistics tracking. When enabled, a message with some
  # statistics is logged for every query.
  # CLI flag: -frontend.query-stats-enabled
  query_stats_enabled: true

  # Number of concurrent workers forwarding queries to single query-scheduler.
  # CLI flag: -frontend.scheduler-worker-concurrency
  scheduler_worker_concurrency: 5

  # # URL of downstream Loki.
  # # CLI flag: -frontend.downstream-url
  # [downstream_url: <string> | default = ""]


  # cache_config:
  #  embedded_cache

# The frontend_worker configures the worker - running within the Loki querier - picking up and executing queries enqueued by the query-frontend.
frontend_worker:
  # Force worker concurrency to match the -querier.max-concurrent option.
  # Overrides querier.worker-parallelism.
  # CLI flag: -querier.worker-match-max-concurrent
  match_max_concurrent: true

  # # Address of query frontend service, in host:port format. If
  # # -querier.scheduler-address is set as well, querier will use scheduler instead.
  # # Only one of -querier.frontend-address or -querier.scheduler-address can be
  # # set. If neither is set, queries are only received via HTTP endpoint.
  # # CLI flag: -querier.frontend-address
  # [frontend_address: <string> | default = ""]

  # # Hostname (and port) of scheduler that querier will periodically resolve,
  # # connect to and receive queries from. Only one of -querier.frontend-address or
  # # -querier.scheduler-address can be set. If neither is set, queries are only
  # # received via HTTP endpoint.
  # # CLI flag: -querier.scheduler-address
  # [scheduler_address: <string> | default = ""]

  # Number of simultaneous queries to process per query-frontend or
  # query-scheduler.
  # CLI flag: -querier.worker-parallelism
  parallelism: 10

  # The grpc_client block configures the gRPC client used to communicate between
  # two Loki components.
  # The CLI flags prefix for this block configuration is: querier.frontend-client
  grpc_client_config:
    max_send_msg_size: 104857600

query_scheduler:
  max_outstanding_requests_per_tenant: 1024
  use_scheduler_ring: true
  scheduler_ring:
    kvstore:
      store: memberlist

querier:
  max_concurrent: 25
  # worker_match_max_concurrent: true
  engine:
    timeout: 300s
    max_look_back_period: 2m
  # Time to wait before sending more than the minimum successful query requests.
  # CLI flag: -querier.extra-query-delay
  extra_query_delay: 0s
  # Maximum lookback beyond which queries are not sent to ingester. 0 means all
  # queries are sent to ingester.
  # CLI flag: -querier.query-ingesters-within
  query_ingesters_within: 3h
  # Maximum duration for which the live tailing requests are served.
  # CLI flag: -querier.tail-max-duration
  tail_max_duration: 1h

ingester:
  autoforget_unhealthy: true
  lifecycler:
    join_after: 10s
    observe_period: 5s
    ring:
      replication_factor: 3
      kvstore:
        store: memberlist
    final_sleep: 0s
  # chunk_idle_period: 1m

  # How long chunks should sit in-memory with no updates before being flushed if
  # they don't hit the max block size. This means that half-empty chunks will
  # still be flushed after a certain period as long as they receive no further
  # activity.
  # CLI flag: -ingester.chunks-idle-period
  chunk_idle_period: 2h
  wal:
    enabled: true
    dir: /loki/wal
  # max_chunk_age: 1m
  # The maximum duration of a timeseries chunk in memory. If a timeseries runs for
  # longer than this, the current chunk will be flushed to the store and a new
  # chunk created.
  # CLI flag: -ingester.max-chunk-age
  max_chunk_age: 2h
  # How long chunks should be retained in-memory after they've been flushed.
  # CLI flag: -ingester.chunks-retain-period
  chunk_retain_period: 30s
  chunk_encoding: snappy
  # chunk_target_size: 1.572864e+06

  # A target _compressed_ size in bytes for chunks. This is a desired size not an
  # exact size, chunks may be slightly bigger or significantly smaller if they get
  # flushed for other reasons (e.g. chunk_idle_period). A value of 0 creates
  # chunks with a fixed 10 blocks, a non zero value will create chunks with a
  # variable number of blocks to meet the target size.
  # CLI flag: -ingester.chunk-target-size
  chunk_target_size: 1536000
  # # The targeted _uncompressed_ size in bytes of a chunk block When this threshold
  # is exceeded the head block will be cut and compressed inside the chunk.
  # CLI flag: -ingester.chunks-block-size
  chunk_block_size: 262144
  # The timeout before a flush is cancelled.
  # CLI flag: -ingester.flush-op-timeout
  flush_op_timeout: 10s

  # # How far back should an ingester be allowed to query the store for data, for
  # # use only with boltdb-shipper/tsdb index and filesystem object store. -1 for
  # # infinite.
  # # CLI flag: -ingester.query-store-max-look-back-period
  # [query_store_max_look_back_period: <duration> | default = 0s]

# https://grafana.com/docs/loki/latest/configuration/#compactor
compactor:
  retention_enabled: true
  # Directory where files can be downloaded for compaction.
  # CLI flag: -boltdb.shipper.compactor.working-directory
  working_directory: /tmp/compactor
  shared_store: s3

  # Maximum number of tables to compact in parallel. While increasing this value,
  # please make sure compactor has enough disk space allocated to be able to store
  # and compact as many tables.
  # CLI flag: -boltdb.shipper.compactor.max-compaction-parallelism
  max_compaction_parallelism: 1

  # Number of upload/remove operations to execute in parallel when finalizing a
  # compaction. NOTE: This setting is per compaction operation, which can be
  # executed in parallel. The upper bound on the number of concurrent uploads is
  # upload_parallelism * max_compaction_parallelism.
  # CLI flag: -boltdb.shipper.compactor.upload-parallelism
  upload_parallelism: 10

  # The hash ring configuration used by compactors to elect a single instance for
  # running compactions. The CLI flags prefix for this block config is:
  # boltdb.shipper.compactor.ring
  compactor_ring:
    kvstore:
      # Backend storage to use for the ring. Supported values are: consul, etcd,
      # inmemory, memberlist, multi.
      # CLI flag: -boltdb.shipper.compactor.ring.store
      store: memberlist


ingester_client:
  grpc_client_config:
    max_recv_msg_size: 104857600  # 100 Mb
    max_send_msg_size: 104857600  # 100 Mb

table_manager:
    # Periodic tables grace period (duration which table will be created/deleted
    # before/after it's needed).
    # CLI flag: -table-manager.periodic-table.grace-period
    creation_grace_period: 3h
    # How frequently to poll backend to learn our capacity.
    # CLI flag: -table-manager.poll-interval
    poll_interval: 10m
    # If true, enables retention deletes of DB tables
    # CLI flag: -table-manager.retention-deletes-enabled
    retention_deletes_enabled: false
    # Tables older than this retention period are deleted. Must be either 0
    # (disabled) or a multiple of 24h. When enabled, be aware this setting is
    # destructive to data!
    # CLI flag: -table-manager.retention-period
    retention_period: 0

# table_manager:
#   retention_deletes_enabled: true
#   retention_period: 336h